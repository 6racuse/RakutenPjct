{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-24T19:08:31.035628Z",
     "start_time": "2024-05-24T19:08:29.860179Z"
    }
   },
   "outputs": [],
   "source": [
    "import header\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "fast_coeff = 10\n",
    "\n",
    "X_train = pd.read_csv(\n",
    "        \"/Users/welto/Library/CloudStorage/OneDrive-CentraleSupelec/2A/CASA/RakutenPjct/data/X_train_update.csv\",\n",
    "        sep=',',\n",
    "        usecols=lambda column: column not in [\n",
    "            'Unnamed: 0',\n",
    "            'imageid',\n",
    "            'description'\n",
    "        ]\n",
    "    )\n",
    "X_test_challenge = pd.read_csv(\n",
    "    \"/Users/welto/Library/CloudStorage/OneDrive-CentraleSupelec/2A/CASA/RakutenPjct/data/X_test_update.csv\",\n",
    "    sep=',',\n",
    "    usecols=lambda column: column not in [\n",
    "        'Unnamed: 0',\n",
    "        'imageid',\n",
    "        'description'\n",
    "    ]\n",
    ")\n",
    "Y_train = pd.read_csv(\n",
    "    \"/Users/welto/Library/CloudStorage/OneDrive-CentraleSupelec/2A/CASA/RakutenPjct/data/Y_train_CVw08PX.csv\",\n",
    "    sep=',',\n",
    "    usecols=lambda column: column != 'Unnamed: 0'\n",
    ")\n",
    "\n",
    "X_train = X_train['designation'][:X_train.shape[0]//fast_coeff].tolist()\n",
    "X_test_challenge = X_test_challenge['designation'][:X_test_challenge.shape[0]//fast_coeff].tolist()\n",
    "\n",
    "Y_train = Y_train['prdtypecode'][:Y_train.shape[0]//fast_coeff].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T19:17:27.253287Z",
     "start_time": "2024-05-24T19:17:26.964971Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_raw_designation_clean : |██████████████████████████████████████████████████| 100.0% Complete\r\n",
      "X_test_raw_designation_clean : |██████████████████████████████████████████████████| 100.0% Complete\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/welto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_punct'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 47\u001B[0m\n\u001B[1;32m     38\u001B[0m     X_test_challenge_clean\u001B[38;5;241m.\u001B[39mappend(tokens)\n\u001B[1;32m     39\u001B[0m     header\u001B[38;5;241m.\u001B[39mprogress_bar(\n\u001B[1;32m     40\u001B[0m         k \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     41\u001B[0m         b,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     44\u001B[0m         length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m\n\u001B[1;32m     45\u001B[0m     )\n\u001B[0;32m---> 47\u001B[0m X_train_strings \u001B[38;5;241m=\u001B[39m [k \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(tokens) \u001B[38;5;28;01mfor\u001B[39;00m tokens \u001B[38;5;129;01min\u001B[39;00m X_train_clean] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_punct\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m k\u001B[38;5;241m.\u001B[39mis_stop]\n\u001B[1;32m     48\u001B[0m X_test_strings \u001B[38;5;241m=\u001B[39m [k \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(tokens) \u001B[38;5;28;01mfor\u001B[39;00m tokens \u001B[38;5;129;01min\u001B[39;00m X_test_challenge_clean] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m k\u001B[38;5;241m.\u001B[39mis_punct \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m k\u001B[38;5;241m.\u001B[39mis_stop]\n\u001B[1;32m     50\u001B[0m preprocessing_end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'is_punct'"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "X_train_clean = []\n",
    "X_test_challenge_clean = []\n",
    "\n",
    "a = len(X_train)\n",
    "b = len(X_test_challenge)\n",
    "\n",
    "header.progress_bar(\n",
    "    0,\n",
    "    a,\n",
    "    prefix='Progress:',\n",
    "    suffix='Complete',\n",
    "    length=50\n",
    ")\n",
    "\n",
    "preprocessing_start_time = time.time()\n",
    "\n",
    "for k in range(a):\n",
    "    tokens = word_tokenize(\n",
    "        header.normalize_accent(X_train[k].lower()),\n",
    "        language='french'\n",
    "    )\n",
    "    X_train_clean.append(tokens)\n",
    "    header.progress_bar(\n",
    "        k + 1,\n",
    "        a,\n",
    "        prefix='X_train_raw_designation_clean :',\n",
    "        suffix='Complete',\n",
    "        length=50\n",
    "    )\n",
    "\n",
    "for k in range(b):\n",
    "    tokens = word_tokenize(\n",
    "        header.normalize_accent(X_test_challenge[k].lower()),\n",
    "        language='french'\n",
    "    )\n",
    "    X_test_challenge_clean.append(tokens)\n",
    "    header.progress_bar(\n",
    "        k + 1,\n",
    "        b,\n",
    "        prefix='X_test_raw_designation_clean :',\n",
    "        suffix='Complete',\n",
    "        length=50\n",
    "    )\n",
    "\n",
    "X_train_strings = [' '.join(tokens) for tokens in X_train_clean]\n",
    "X_test_strings = [' '.join(tokens) for tokens in X_test_challenge_clean]\n",
    "\n",
    "preprocessing_end_time = time.time()\n",
    "preprocessing_time_h, preprocessing_time_min, preprocessing_time_s = header.convert_seconds(\n",
    "    preprocessing_end_time - preprocessing_start_time)\n",
    "\n",
    "print(f\"Preprocessed in {int(preprocessing_time_h)}h {int(preprocessing_time_min)}min {int(preprocessing_time_s)}s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T19:24:02.690671Z",
     "start_time": "2024-05-24T19:24:02.667405Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T19:16:52.958735Z",
     "start_time": "2024-05-24T19:16:52.953519Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_strings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_raw_designation_clean : |██████████████████████████████████████████████████| 100.0% Complete\r\n",
      "X_test_raw_designation_clean : |██████████████████████████████████████████████████| 100.0% Complete\r\n",
      "Preprocessed in 0h 0min 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/welto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/welto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import time\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "X_train_clean = []\n",
    "X_test_challenge_clean = []\n",
    "\n",
    "a = 10 #len(X_train)\n",
    "b = 10 #len(X_test_challenge)\n",
    "\n",
    "def progress_bar(current, total, prefix='', suffix='', length=50):\n",
    "    percent = (\"{0:.1f}\").format(100 * (current / float(total)))\n",
    "    filled_length = int(length * current // total)\n",
    "    bar = '█' * filled_length + '-' * (length - filled_length)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end='\\r')\n",
    "    if current == total:\n",
    "        print()\n",
    "\n",
    "preprocessing_start_time = time.time()\n",
    "\n",
    "for k in range(a):\n",
    "    tokens = word_tokenize(header.normalize_accent(X_train[k].lower()), language='french')\n",
    "    tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words]\n",
    "    X_train_clean.append(tokens)\n",
    "    progress_bar(k + 1, a, prefix='X_train_raw_designation_clean :', suffix='Complete', length=50)\n",
    "\n",
    "for k in range(b):\n",
    "    tokens = word_tokenize(header.normalize_accent(X_test_challenge[k].lower()), language='french')\n",
    "    tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words]\n",
    "    X_test_challenge_clean.append(tokens)\n",
    "    progress_bar(k + 1, b, prefix='X_test_raw_designation_clean :', suffix='Complete', length=50)\n",
    "\n",
    "X_train_strings = [' '.join(tokens) for tokens in X_train_clean]\n",
    "X_test_strings = [' '.join(tokens) for tokens in X_test_challenge_clean]\n",
    "\n",
    "preprocessing_end_time = time.time()\n",
    "preprocessing_time_h, preprocessing_time_min, preprocessing_time_s = header.convert_seconds(\n",
    "    preprocessing_end_time - preprocessing_start_time)\n",
    "\n",
    "print(f\"Preprocessed in {int(preprocessing_time_h)}h {int(preprocessing_time_min)}min {int(preprocessing_time_s)}s\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T19:58:43.698581Z",
     "start_time": "2024-05-24T19:58:43.688978Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "['olivia personalisiertes notizbuch 150 seiten punktraster ca din a5 rosen-design',\n \"journal arts 133 28/09/2001 l'art marche salon d'art asiatique a paris jacques barrere francois perrier reforme ventes encheres publiques sna fete cent ans\",\n 'grand stylet ergonomique bleu gamepad nintendo wii u speedlink pilot style',\n 'peluche donald europe disneyland 2000 marionnette a doigt',\n 'guerre tuques',\n 'afrique contemporaine 212 hiver 2004 dossier japon afrique',\n 'christof e bildungsprozessen auf der spur',\n 'conquerant sept cahier couverture polypro 240 x 320 mm 96 pages 90g seyes incolore',\n 'puzzle scooby-doo poster 2x35 pieces',\n 'tente pliante v3s5-pro pvc blanc 3 x 4m50 longueur 4m50 largeur 3 blanc h']"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_strings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T19:58:46.481156Z",
     "start_time": "2024-05-24T19:58:46.477461Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 5.        ,  5.10204082,  5.20408163,  5.30612245,  5.40816327,\n        5.51020408,  5.6122449 ,  5.71428571,  5.81632653,  5.91836735,\n        6.02040816,  6.12244898,  6.2244898 ,  6.32653061,  6.42857143,\n        6.53061224,  6.63265306,  6.73469388,  6.83673469,  6.93877551,\n        7.04081633,  7.14285714,  7.24489796,  7.34693878,  7.44897959,\n        7.55102041,  7.65306122,  7.75510204,  7.85714286,  7.95918367,\n        8.06122449,  8.16326531,  8.26530612,  8.36734694,  8.46938776,\n        8.57142857,  8.67346939,  8.7755102 ,  8.87755102,  8.97959184,\n        9.08163265,  9.18367347,  9.28571429,  9.3877551 ,  9.48979592,\n        9.59183673,  9.69387755,  9.79591837,  9.89795918, 10.        ])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(5, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T20:34:42.190119Z",
     "start_time": "2024-05-24T20:34:42.174831Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/welto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/welto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: |████████████████----------------------------------| 32.4% Complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 123\u001B[0m\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuted in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mint\u001B[39m(exec_time_h)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mh \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mint\u001B[39m(exec_time_min)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mmin \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mint\u001B[39m(exec_time_s)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 123\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfast_coeff\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[36], line 113\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(fast_coeff)\u001B[0m\n\u001B[1;32m    105\u001B[0m svm \u001B[38;5;241m=\u001B[39m SVC(C\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8.071428571428571\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrbf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    107\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m Pipeline([\n\u001B[1;32m    108\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpreprocessor\u001B[39m\u001B[38;5;124m'\u001B[39m, text_preprocessor),\n\u001B[1;32m    109\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvectorizer\u001B[39m\u001B[38;5;124m'\u001B[39m, tfidf_vectorizer),\n\u001B[1;32m    110\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclassifier\u001B[39m\u001B[38;5;124m'\u001B[39m, svm)\n\u001B[1;32m    111\u001B[0m ])\n\u001B[0;32m--> 113\u001B[0m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m Y_pred_challenge \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mpredict(X_test_challenge)\n\u001B[1;32m    116\u001B[0m header\u001B[38;5;241m.\u001B[39mSave_label_output(Y_pred_challenge, \u001B[38;5;28mlen\u001B[39m(X_train))\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1472\u001B[0m     )\n\u001B[1;32m   1473\u001B[0m ):\n\u001B[0;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py:475\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m    473\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    474\u001B[0m         last_step_params \u001B[38;5;241m=\u001B[39m routed_params[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]]\n\u001B[0;32m--> 475\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_final_estimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mlast_step_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfit\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1472\u001B[0m     )\n\u001B[1;32m   1473\u001B[0m ):\n\u001B[0;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_base.py:250\u001B[0m, in \u001B[0;36mBaseLibSVM.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    247\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LibSVM]\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    249\u001B[0m seed \u001B[38;5;241m=\u001B[39m rnd\u001B[38;5;241m.\u001B[39mrandint(np\u001B[38;5;241m.\u001B[39miinfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmax)\n\u001B[0;32m--> 250\u001B[0m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msolver_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001B[39;00m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape_fit_ \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m (n_samples,)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_base.py:370\u001B[0m, in \u001B[0;36mBaseLibSVM._sparse_fit\u001B[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001B[0m\n\u001B[1;32m    356\u001B[0m kernel_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sparse_kernels\u001B[38;5;241m.\u001B[39mindex(kernel)\n\u001B[1;32m    358\u001B[0m libsvm_sparse\u001B[38;5;241m.\u001B[39mset_verbosity_wrap(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[1;32m    360\u001B[0m (\n\u001B[1;32m    361\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupport_,\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupport_vectors_,\n\u001B[1;32m    363\u001B[0m     dual_coef_data,\n\u001B[1;32m    364\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintercept_,\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_support,\n\u001B[1;32m    366\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_probA,\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_probB,\n\u001B[1;32m    368\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_status_,\n\u001B[1;32m    369\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_iter,\n\u001B[0;32m--> 370\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[43mlibsvm_sparse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlibsvm_sparse_train\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    373\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindptr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m    \u001B[49m\u001B[43msolver_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkernel_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdegree\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoef0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mclass_weight_\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshrinking\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    389\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprobability\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warn_from_fit_status()\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclasses_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32msklearn/svm/_libsvm_sparse.pyx:219\u001B[0m, in \u001B[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/scipy/sparse/_compressed.py:27\u001B[0m, in \u001B[0;36m_cs_matrix.__init__\u001B[0;34m(self, arg1, shape, dtype, copy)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01m_cs_matrix\u001B[39;00m(_data_matrix, _minmax_mixin, IndexMixin):\n\u001B[1;32m     23\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m    base array/matrix class for compressed row- and column-oriented arrays/matrices\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, arg1, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     28\u001B[0m         _data_matrix\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m issparse(arg1):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T13:19:03.970341Z",
     "start_time": "2024-05-25T13:14:17.710078Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Olivia: Personalisiertes Notizbuch / 150 Seiten / Punktraster / Ca Din A5 / Rosen-Design\n",
      "Preprocessed text:   (0, 539)\t0.3037767415332776\n",
      "  (0, 523)\t0.3037767415332776\n",
      "  (0, 498)\t0.3037767415332776\n",
      "  (0, 450)\t0.3037767415332776\n",
      "  (0, 423)\t0.3037767415332776\n",
      "  (0, 415)\t0.3037767415332776\n",
      "  (0, 216)\t0.3037767415332776\n",
      "  (0, 211)\t0.2778433246170265\n",
      "  (0, 143)\t0.3037767415332776\n",
      "  (0, 80)\t0.3037767415332776\n",
      "  (0, 17)\t0.3037767415332776\n",
      "\n",
      "Original text: Journal Des Arts (Le) N° 133 Du 28/09/2001 - L'art Et Son Marche Salon D'art Asiatique A Paris - Jacques Barrere - Francois Perrier - La Reforme Des Ventes Aux Encheres Publiques - Le Sna Fete Ses Cent Ans.\n",
      "Preprocessed text:   (0, 613)\t0.20310216736249212\n",
      "  (0, 554)\t0.20310216736249212\n",
      "  (0, 530)\t0.20310216736249212\n",
      "  (0, 511)\t0.20310216736249212\n",
      "  (0, 495)\t0.20310216736249212\n",
      "  (0, 449)\t0.20310216736249212\n",
      "  (0, 438)\t0.16391899568759463\n",
      "  (0, 372)\t0.20310216736249212\n",
      "  (0, 334)\t0.20310216736249212\n",
      "  (0, 321)\t0.20310216736249212\n",
      "  (0, 271)\t0.20310216736249212\n",
      "  (0, 258)\t0.20310216736249212\n",
      "  (0, 241)\t0.20310216736249212\n",
      "  (0, 154)\t0.20310216736249212\n",
      "  (0, 116)\t0.18576333768046813\n",
      "  (0, 100)\t0.20310216736249212\n",
      "  (0, 99)\t0.20310216736249212\n",
      "  (0, 98)\t0.34692248448450475\n",
      "  (0, 96)\t0.20310216736249212\n",
      "  (0, 40)\t0.20310216736249212\n",
      "  (0, 28)\t0.20310216736249212\n",
      "  (0, 13)\t0.20310216736249212\n",
      "  (0, 4)\t0.18576333768046813\n",
      "\n",
      "Original text: Grand Stylet Ergonomique Bleu Gamepad Nintendo Wii U - Speedlink Pilot Style\n",
      "Preprocessed text:   (0, 630)\t0.327101582583282\n",
      "  (0, 570)\t0.327101582583282\n",
      "  (0, 569)\t0.2991769242559803\n",
      "  (0, 559)\t0.327101582583282\n",
      "  (0, 459)\t0.2991769242559803\n",
      "  (0, 411)\t0.327101582583282\n",
      "  (0, 289)\t0.327101582583282\n",
      "  (0, 275)\t0.2991769242559803\n",
      "  (0, 247)\t0.327101582583282\n",
      "  (0, 129)\t0.2991769242559803\n",
      "\n",
      "Original text: Peluche Donald - Europe - Disneyland 2000 (Marionnette À Doigt)\n",
      "Preprocessed text:   (0, 448)\t0.3824560756607918\n",
      "  (0, 374)\t0.3824560756607918\n",
      "  (0, 249)\t0.3498058048987754\n",
      "  (0, 222)\t0.3824560756607918\n",
      "  (0, 221)\t0.3824560756607918\n",
      "  (0, 218)\t0.3824560756607918\n",
      "  (0, 27)\t0.3824560756607918\n",
      "\n",
      "Original text: La Guerre Des Tuques\n",
      "Preprocessed text:   (0, 598)\t0.7071067811865476\n",
      "  (0, 292)\t0.7071067811865476\n",
      "\n",
      "Executed in 0h 0min 0s\n"
     ]
    }
   ],
   "source": [
    "import header\n",
    "\n",
    "import string\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from joblib import dump, load\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def download_nltk_data():\n",
    "    if not nltk.data.find('tokenizers/punkt'):\n",
    "        nltk.download(\"punkt\")\n",
    "    if not nltk.data.find('corpora/stopwords'):\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "def load_data(fast_coeff : int):\n",
    "    X_train = pd.read_csv(\n",
    "        \"/Users/welto/Library/CloudStorage/OneDrive-CentraleSupelec/2A/CASA/RakutenPjct/data/X_train_update.csv\",\n",
    "        sep=',',\n",
    "        usecols=lambda column: column not in [\n",
    "            'Unnamed: 0',\n",
    "            'imageid',\n",
    "            'description'\n",
    "        ]\n",
    "    )\n",
    "    X_test_challenge = pd.read_csv(\n",
    "        \"/Users/welto/Library/CloudStorage/OneDrive-CentraleSupelec/2A/CASA/RakutenPjct/data/X_test_update.csv\",\n",
    "        sep=',',\n",
    "        usecols=lambda column: column not in [\n",
    "            'Unnamed: 0',\n",
    "            'imageid',\n",
    "            'description'\n",
    "        ]\n",
    "    )\n",
    "    Y_train = pd.read_csv(\n",
    "        \"/Users/welto/Library/CloudStorage/OneDrive-CentraleSupelec/2A/CASA/RakutenPjct/data/Y_train_CVw08PX.csv\",\n",
    "        sep=',',\n",
    "        usecols=lambda column: column != 'Unnamed: 0'\n",
    "    )\n",
    "\n",
    "    X_train = X_train['designation'][:X_train.shape[0] // fast_coeff].tolist()\n",
    "    X_test_challenge = X_test_challenge['designation'][:X_test_challenge.shape[0] // fast_coeff].tolist()\n",
    "\n",
    "    Y_train = Y_train['prdtypecode'][:Y_train.shape[0] // fast_coeff].tolist()\n",
    "\n",
    "    return X_train, X_test_challenge, Y_train\n",
    "\n",
    "def preprocess_text(text):\n",
    "    download_nltk_data()\n",
    "\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "\n",
    "    tokens = word_tokenize(\n",
    "        header.normalize_accent(\n",
    "            text.lower()\n",
    "        ),\n",
    "        language='french'\n",
    "    )\n",
    "    tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def train_model(\n",
    "        X_train,\n",
    "        Y_train\n",
    "):\n",
    "    params = {\n",
    "        'C': 8.071428571428571,\n",
    "        'gamma': 0.1,\n",
    "        'kernel': 'rbf'\n",
    "    }\n",
    "    pipeline = Pipeline([\n",
    "        (\n",
    "            'preprocessor',\n",
    "            TfidfVectorizer(\n",
    "                preprocessor=preprocess_text\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            'classifier',\n",
    "            SVC(\n",
    "                C=params['C'],\n",
    "                gamma=params['gamma'],\n",
    "                kernel=params['kernel']\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(\n",
    "        X_train,\n",
    "        Y_train\n",
    "    )\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def main(fast_coeff : int):\n",
    "    exec_time_start = time.time()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    X_train, X_test_challenge, Y_train = load_data(fast_coeff)\n",
    "\n",
    "    model = train_model(X_train, Y_train)\n",
    "\n",
    "    # Access the preprocessed data\n",
    "    X_train_preprocessed = model.named_steps['preprocessor'].transform(X_train)\n",
    "\n",
    "    # Print out the preprocessed data for the first few samples\n",
    "    for i in range(5):  # Print the first 5 samples\n",
    "        print(f\"Original text: {X_train[i]}\")\n",
    "        print(f\"Preprocessed text: {X_train_preprocessed[i]}\\n\")\n",
    "\n",
    "    Y_pred = model.predict(X_test_challenge)\n",
    "\n",
    "    header.Save_label_output(Y_pred, len(X_train))\n",
    "\n",
    "    exec_time_end = time.time()\n",
    "    exec_time_h, exec_time_min, exec_time_s = header.convert_seconds(exec_time_end - exec_time_start)\n",
    "\n",
    "    print(\n",
    "        f\"Executed in {int(exec_time_h)}h {int(exec_time_min)}min {int(exec_time_s)}s\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(fast_coeff=1000)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T15:48:23.910962Z",
     "start_time": "2024-05-25T15:48:23.428724Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
