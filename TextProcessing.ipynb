{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aH2vQI6EDyhG"
   },
   "source": [
    "<h6><center>Introduction to Data Sciences</center></h6>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnPwa1GSDyhH"
   },
   "source": [
    "## Lab 4: Text Mining\n",
    "\n",
    "Text mining is the process of automatically extracting \"high-quality\" information from text. High-quality information is typically derived by transforming the free (unstructured) text in documents and databases into normalized, structured data suitable for analysis or to drive machine learning (ML) algorithms.\n",
    "\n",
    "Text mining identifies facts, relationships and assertions that would otherwise remain buried in the mass of textual big data. Once extracted, this information is converted into a structured form that can be further analyzed, or presented directly using clustered HTML tables, mind maps, charts, etc. Text mining employs a variety of methodologies to process the text, one of the most important of these being Natural Language Processing (NLP).\n",
    "\n",
    "Typical text mining applications include:\n",
    "- Text classification\n",
    "    - e.g. spam email detection\n",
    "- Text clustering\n",
    "    - e.g. document retrieval, topic extraction\n",
    "- Sentiment analysis\n",
    "    - e.g. detecting the emotions like \"angry\", \"sad\", and \"happy\" in a customer message\n",
    "- Named entity recognition, etc.\n",
    "    - e.g. detecting person names, organizations, locations in texts\n",
    "\n",
    "## In this lab we will learn:\n",
    "1. Preprocessing: textual normalization, simple tokenization, stopword removal\n",
    "2. Converting documents into feature sets: Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sToeuWaCDyhI"
   },
   "source": [
    "---\n",
    "## Text Normalization and Preprocessing\n",
    "\n",
    "Text normalization is the process of transforming text into a single canonical form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T18:52:32.917406Z",
     "start_time": "2024-05-24T18:52:32.910613Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_progress_bar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oVqPHobDyhL"
   },
   "source": [
    "### Lower case\n",
    "A computer does not **require** upper case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sFX_v8lIDyhL",
    "ExecuteTime": {
     "end_time": "2024-05-24T18:52:33.766166Z",
     "start_time": "2024-05-24T18:52:33.761375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la pie niche haut. l’oie niche bas. où l’hibou niche-t-il ?l’hibou niche ni haut ni bas. l'hibou niche pas.\n"
     ]
    }
   ],
   "source": [
    "# An example string:\n",
    "raw_1 = \"La pie niche haut. L’oie niche bas. Où l’hibou niche-t-il ?L’hibou niche ni haut ni bas. L'hibou niche pas.\"\n",
    "\n",
    "# Write code to lower case the string\n",
    "s = raw_1.lower()\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4fKP-sCDyhN"
   },
   "source": [
    "### Handling Accented Characters\n",
    "\n",
    "Diacritics or accents on characters in English have a fairly marginal status, and we might well want `cliché` and `cliche` to match, or `naive` and `naïve`. This can be done by normalizing tokens to remove diacritics. In many other languages, diacritics are a regular part of the writing system and distinguish different sounds. Occasionally words are distinguished only by their accents. For instance, in French, `pêche` is `fishinig` while `péché` is `sin`.\n",
    "\n",
    "Nevertheless, the important question is usually not prescriptive or linguistic but is a question of how users are likely to write queries for these words. In many cases, users will enter queries for words without diacritics, whether for reasons of speed, laziness, limited software, or habits born of the days when it was hard to use non-ASCII text on many computer systems. In these cases, it might be best to equate all words to a form without diacritics.\n",
    "\n",
    "We will simply list all French accents and use string replace to convert accented characters with their canonical form.\n",
    "\n",
    "Let's replace accented characters with their canonical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2KlfcoBQDyhO",
    "ExecuteTime": {
     "end_time": "2024-05-24T18:52:34.519604Z",
     "start_time": "2024-05-24T18:52:34.485346Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_accent(string):\n",
    "    string = string.replace('á', 'a')\n",
    "    string = string.replace('â', 'a')\n",
    "\n",
    "    string = string.replace('é', 'e')\n",
    "    string = string.replace('è', 'e')\n",
    "    string = string.replace('ê', 'e')\n",
    "    string = string.replace('ë', 'e')\n",
    "\n",
    "    string = string.replace('î', 'i')\n",
    "    string = string.replace('ï', 'i')\n",
    "\n",
    "    string = string.replace('ö', 'o')\n",
    "    string = string.replace('ô', 'o')\n",
    "    string = string.replace('ò', 'o')\n",
    "    string = string.replace('ó', 'o')\n",
    "\n",
    "    string = string.replace('ù', 'u')\n",
    "    string = string.replace('û', 'u')\n",
    "    string = string.replace('ü', 'u')\n",
    "\n",
    "    string = string.replace('ç', 'c')\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yKqV7dv5DyhO",
    "ExecuteTime": {
     "end_time": "2024-05-24T18:52:34.895817Z",
     "start_time": "2024-05-24T18:52:34.888485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La pie niche haut. L’oie niche bas. Ou l’hibou niche-t-il ?L’hibou niche ni haut ni bas. L'hibou niche pas.\n"
     ]
    }
   ],
   "source": [
    "print(normalize_accent(raw_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elJ0W-YVDyhP"
   },
   "source": [
    "### Tokenization : Spacy\n",
    "[spaCy](https://spacy.io/usage) is a platform to work with natural language data using Python.\n",
    "\n",
    "We will work with French data, so **you need to install proper the proper language package for French**. The complete installation instructions are [available in this link](https://spacy.io/usage).\n",
    "\n",
    "\n",
    "\n",
    "As usual, we will first convert everything to lowercase and normalize accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K1KuXJH7DyhR",
    "ExecuteTime": {
     "end_time": "2024-05-24T18:52:35.923131Z",
     "start_time": "2024-05-24T18:52:35.918085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latte otee, mur gate, trou s’y fit, rat s’y mit, chat l’y vit, chat l’y prit.\n"
     ]
    }
   ],
   "source": [
    "raw_2 = \"Latte ôtée, mur gâté, trou s’y fit, rat s’y mit, chat l’y vit, chat l’y prit.\"\n",
    "\n",
    "# Write code here to convert everything in lower case and to normalize accents.\n",
    "s = normalize_accent(raw_2)\n",
    "s = s.lower()\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpXeDrzMDyhR"
   },
   "source": [
    "`spaCy` already provides us with modules to easily tokenize the text.\n",
    "\n",
    "For that, first spaCy needs to be loaded with the required language. Then we can use the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:10:14.851487Z",
     "start_time": "2024-05-15T14:10:05.954960Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.7.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.3/16.3 MB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fr-core-news-sm==3.7.0) (3.7.4)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.3)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.4)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.4.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.7.1)\r\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\r\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (69.2.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.11.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.1)\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:10:18.589744Z",
     "start_time": "2024-05-15T14:10:16.190327Z"
    },
    "id": "iPdpSL6tDyhS"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy for french\n",
    "spacy_nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nT0a7VcTDyhS"
   },
   "source": [
    "[Here is a tutorial](https://towardsdatascience.com/a-short-introduction-to-nlp-in-python-with-spacy-d0aa819af3ad) for basic usages of spaCy.\n",
    "\n",
    "Upon running spaCy on a string, it automatically generates spaCy-token object. We simply need to get the string form (also called the orthogonal form) of the tokens. We will use `token.orth_` to get the string form of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:10:19.459850Z",
     "start_time": "2024-05-15T14:10:19.425620Z"
    },
    "id": "DxV9FIpYDyhS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['latte', 'otee', ',', 'mur', 'gate', ',', 'trou', 's’', 'y', 'fit', ',', 'rat', 's’', 'y', 'mit', ',', 'chat', 'l’', 'y', 'vit', ',', 'chat', 'l’', 'y', 'prit', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "spacy_tokens = spacy_nlp(s)\n",
    "string_tokens = [token.orth_ for token in spacy_tokens]\n",
    "\n",
    "print(string_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMl7ohUnDyhT"
   },
   "source": [
    "### Handling Punctuations\n",
    "\n",
    "Punctuations are noises while processing text. We are more interesting in the words itself.\n",
    "\n",
    "SpaCy recognises punctuation and is able to split these punctuation tokens from word tokens. We can use the `token.is_punct` to identify a punctuation token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:10:20.550406Z",
     "start_time": "2024-05-15T14:10:20.545790Z"
    },
    "id": "Ng6D7bYnDyhT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['latte', 'otee', 'mur', 'gate', 'trou', 's’', 'y', 'fit', 'rat', 's’', 'y', 'mit', 'chat', 'l’', 'y', 'vit', 'chat', 'l’', 'y', 'prit']\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation tokens\n",
    "string_tokens = [token.orth_ for token in spacy_tokens if not token.is_punct]\n",
    "\n",
    "print(string_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fj6CM3uUDyhT"
   },
   "source": [
    "### Stop word filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB-L9cZpDyhT"
   },
   "source": [
    "Stop words are words which are filtered out before or after processing of natural language data (text). There is no universal stop-word list. Often, stop word lists include short function words, such as \"the\", \"is\", \"at\", \"which\", \"on\" etc. in English and \"le\", \"la\", \"et\", \"à\", \"qui\" etc. in French. Removing  stop-words has been shown to increase the performance of different tasks like search.\n",
    "\n",
    "Lucky for us, spaCy can also detect if a given token is a stop word. The stop words detection by spaCy will depend on the language we used to load spaCy. In our case it will detect only French stop words.\n",
    "\n",
    "We can use the `token.is_stop` to identify a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:10:22.009001Z",
     "start_time": "2024-05-15T14:10:22.002733Z"
    },
    "id": "SOWbepgLDyhU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['latte', 'otee', 'mur', 'gate', 'trou', 'fit', 'rat', 'mit', 'chat', 'vit', 'chat', 'prit']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "string_tokens = [token.orth_ for token in spacy_tokens if not token.is_punct if not token.is_stop]\n",
    "\n",
    "print(string_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvmqkgJbDyhU"
   },
   "source": [
    "### Lastly, recombining tokens into a string\n",
    "\n",
    "Many applications require to input a string, not a list of tokens. So we can merge the tokens into a single string. This will give us a **clean** string which we got after preprocessing of the raw string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:10:23.682607Z",
     "start_time": "2024-05-15T14:10:23.675653Z"
    },
    "id": "ULvnED8XDyhU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latte otee mur gate trou fit rat mit chat vit chat prit\n"
     ]
    }
   ],
   "source": [
    "# Combining list of tokens into a single string\n",
    "clean_2 = \" \".join(string_tokens)\n",
    "\n",
    "print(clean_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xA_PZcwDyhU"
   },
   "source": [
    "## Let's combine everything: write a function\n",
    "Using above steps, we will now write a function. We will call this function **raw_to_text**. This function will take a raw text string and will return a list of tokens. We will also supply the spacy object for French which we loaded earlier. This will save us time to load the module again and again.\n",
    "1. lower case\n",
    "2. normalize accents (use the method we created before)\n",
    "3. tokenize\n",
    "4. remove punctuation tokens and stop words\n",
    "5. joining the tokens back into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:19:49.406015Z",
     "start_time": "2024-05-15T14:19:49.398867Z"
    },
    "id": "2ldd6kDpDyhV"
   },
   "outputs": [],
   "source": [
    "def raw_to_tokens(raw_string, spacy_nlp):\n",
    "\n",
    "    # Write code for lower-casing\n",
    "    raw_string = raw_string.lower()\n",
    "\n",
    "    # Write code to normalize the accents\n",
    "    raw_string = normalize_accent(raw_string)\n",
    "\n",
    "    # Write code to tokenize\n",
    "    spacy_tokens = spacy_nlp(raw_string)\n",
    "\n",
    "    # Write code to remove punctuation and stop words tokens and create string tokens\n",
    "    string_tokens =  [tokens.orth_ for tokens in spacy_tokens if not tokens.is_punct if not tokens.is_stop]\n",
    "\n",
    "    # Write code to join the tokens back into a single string\n",
    "    clean_string = \" \".join(string_tokens)\n",
    "\n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HegZFOOjDyhV"
   },
   "source": [
    "Let's test the function with some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "za-xV759DyhX",
    "ExecuteTime": {
     "end_time": "2024-05-24T19:00:17.961216Z",
     "start_time": "2024-05-24T19:00:17.486621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '.', 'This', 'is', 'a', 'test', 'sentence', '.']\n",
      "['Hello world.', 'This is a test sentence.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/welto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Now you can use the word_tokenize and sent_tokenize functions\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hello world. This is a test sentence.\"\n",
    "\n",
    "# Tokenizing into words\n",
    "words = word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "# Tokenizing into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1FlD--NDyhY"
   },
   "source": [
    "## Something bigger\n",
    "We will use [**EU Parliament Statements**](https://www.statmt.org/europarl/index.html) dataset. You have been supplied with a small extract of French statements for this lab. Here is an example:\n",
    ">Les critères de choix et les activités subventionnées dans le cadre de Leader atténuent, dans le meilleur des cas, une partie des problèmes de l'espace rural d' importance secondaire, tandis que dans le pire des cas, dégénèrent en un affaiblissement des relations publiques et une corruption des consciences.\n",
    "\n",
    "In this hands-on we will use 10,000 French documents extracted from the English-French bilingual dataset.\n",
    "\n",
    "The file **corpus.txt** supplied here, contains 10,000 documents. Each line of the file is a document.\n",
    "\n",
    "Now we will:\n",
    "   1. Load the documents as a list\n",
    "   2. Pre-process the documents\n",
    "   \n",
    "Note: Each line of the file **corpus.txt** is a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:55:34.721173Z",
     "start_time": "2024-05-15T14:55:34.707200Z"
    },
    "id": "NH-FEtvJDyhY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 documents.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write code to load documents as a list\n",
    "\n",
    "Hint 1: open the file using open()\n",
    "Hint 2: use read() to load the content\n",
    "Hint 3: use splitlines() to get separate documents\n",
    "\n",
    "This will give us a list of strings, each string is document.\n",
    "\"\"\"\n",
    "\n",
    "file = open(\"/Users/welto/Library/CloudStorage/OneDrive-CentraleSupelec/2A/CASA/Data Science/TD:TP/corpus.txt\")\n",
    "docs_raw = file.read()\n",
    "docs_raw = docs_raw.splitlines()\n",
    "\n",
    "print(\"Loaded \" + str(len(docs_raw)) + \" documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:55:36.653318Z",
     "start_time": "2024-05-15T14:55:36.646897Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n"
     ]
    }
   ],
   "source": [
    "print(docs_raw[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:36:02.570678Z",
     "start_time": "2024-05-15T14:35:06.704669Z"
    },
    "id": "ba5p_rSFDyhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\r\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write code to create a list of pre-processed documents\n",
    "Hint: use raw_to_tokens function in a loop for each document\n",
    "\n",
    "Note: this might take few minutes.\n",
    "\"\"\"\n",
    "docs_clean = []\n",
    "\n",
    "i = len(docs_raw)\n",
    "print_progress_bar(0, i, prefix='Progress:', suffix='Complete', length=50)\n",
    "\n",
    "for k in range(i) :\n",
    "    docs_clean.append(\n",
    "        raw_to_tokens(docs_raw[k], spacy_nlp)\n",
    "    )\n",
    "    # Update the progress bar\n",
    "    print_progress_bar(k + 1, i, prefix='Progress:', suffix='Complete', length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:55:40.521351Z",
     "start_time": "2024-05-15T14:55:40.511031Z"
    },
    "id": "gkyKceKIDyha"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw document:  Et ceci aussi parce que je sais qu'outre les experts économiques et financiers, nombre de sociaux-démocrates de cette Assemblée partagent notre critique.\n",
      "Preprocessed document:  sais experts economiques financiers nombre sociaux democrates assemblee partagent critique\n"
     ]
    }
   ],
   "source": [
    "# Print sample documents\n",
    "print(\"Raw document: \", docs_raw[4938])\n",
    "print(\"Preprocessed document: \", docs_clean[4938])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVB1qqF-Dyha"
   },
   "source": [
    "## Vector Space Model\n",
    "We are interested in using this data to build statistical models. So, we now need to **vectorize** this data. The goal is to find a way to represent the data so that the computer can understand it.\n",
    "\n",
    "### Bag of words\n",
    "A bag of words represents each document in a corpus as a series of features. Most commonly, the features are the collection of all unique words in the vocabulary of the entire corpus. The values are usually the count of the number of times that word appears in the document, i.e. **term frequency**.\n",
    "\n",
    "A document $d$ is represented by a weight vector is $v_d=[w_{1,d} , w_{2,d},\\ldots, w_{N,d}]$ where $w_{t,d} = tf_{t,d}$, the term frequency of word $t$ in document $d$.\n",
    "\n",
    "A corpus is then represented as a matrix with one row per document and one column per unique word.\n",
    "\n",
    "### Scikit-Learn\n",
    "[Scikit-learn](http://scikit-learn.org/stable/) is machine learning library for the Python programming language. It features a wide range of machine learning algorithms for classification, regression and clustering. It also provides various supporting machine learning techniques such as cross validation, text vectorizer. Scikit-learn is designed to interoperate with the Python numerical and scientific libraries [NumPy](http://www.numpy.org/).\n",
    "\n",
    "Simple to use: import the required module and call it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETikFYFqDyha"
   },
   "source": [
    "## Vectorizer\n",
    "To build our initial bag of words count matrix, we will use scikit-learn's **CountVectorizer** class to transform our corpus into a bag of words representation. CountVectorizer expects as input a list of raw strings containing the documents in the corpus. It tabulates occurrance counts per document for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:55:44.062676Z",
     "start_time": "2024-05-15T14:55:44.031041Z"
    },
    "id": "ABW1MaOGDyhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Matrix:\n",
      "[[0 1 0 0 0 0 1 0 0 1]\n",
      " [0 1 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 1 1 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 1 0]\n",
      " [1 1 0 0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 1 1 0 0 0]]\n",
      "\n",
      "Words in vocabulary:\n",
      "['brûla' 'chat' 'chaud' 'mit' 'patte' 'quitta' 'rôt' 'tenta' 'trop' 'vit']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "docs_raw_sample = [\"Chat vit rôt.\",\n",
    "                   \"Rôt tenta chat.\",\n",
    "                   \"Chat mit patte à rôt.\",\n",
    "                   \"Rôt trop chaud !\",\n",
    "                   \"Rôt brûla patte à chat.\",\n",
    "                   \"Chat quitta rôt.\"]\n",
    "\n",
    "# Write code to import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Write code to convert the list of documents to list of tokens.\n",
    "\n",
    "docs_raw_sample_tokens = [raw_to_tokens(docs_raw_sample[k], spacy_nlp) for k in range(len(docs_raw_sample))]\n",
    "\n",
    "# Write code to create a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Write code to vectorize the sample text\n",
    "X_sample = vectorizer.fit_transform(docs_raw_sample)\n",
    "\n",
    "# The matrix is to be converted to dense matrix to print it\n",
    "print(\"Count Matrix:\")\n",
    "print(X_sample.todense())\n",
    "print(\"\\nWords in vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUOO2UrQDyhc"
   },
   "source": [
    "## TF-IDF Weighting Scheme\n",
    "The tf-idf weighting scheme is an improvement over the simple term count or term frequency scheme we just saw. It is frequently used in text mining applications and has been shown to be effective. It combines two term statistics components:\n",
    "1. **Local component**: term count or term frequency (tf) reflects how important a word is to a document locally. For more details you can refer to [this link](https://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html).\n",
    "2. **Global component**: inverse document frequency (idf) of a word reflects how important the word is to the entire corpus or collection of documents. _Document frequency_ (df) of a word is the number of documents in the corpus where the word appears. A term with higher $df$ is a common term, thus carries less importance. $idf$ is an inverse function of $df$. So higher $idf$ means higher importance of the term globally. For more details you can refer to [this link](https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html).\n",
    "\n",
    "The weight vector for document $d$ under tf-idf scheme is $v_d=[w_{1,d} , w_{2,d},\\ldots, w_{N,d}]$ where $w_{t,d}=tf_{t,d}\\times\\log\\frac{Card(D)}{Card(d'\\in D | t\\in d') + 1}$ In the denominator we have added 1 to avoid division by zero, which is called smoothing.\n",
    "\n",
    "Scikit-learn has your back, it already provides the **TfidfVectorizer** module to compute TF-IDF matrix.\n",
    "\n",
    "**Note**:\n",
    "1. Scikit-learn uses a slightly different formula than that we saw today morning. You can refer to [corresponding documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) to know more.\n",
    "2. Do not forget about preprocessing and tokenization before doing vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:55:47.829726Z",
     "start_time": "2024-05-15T14:55:47.802319Z"
    },
    "id": "ge2wDfMVDyhc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF Matrix:\n",
      "(6, 10)\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.42407356 0.         0.         0.         0.\n",
      "  0.36743345 0.         0.         0.82774046]\n",
      " [0.         0.42407356 0.         0.         0.         0.\n",
      "  0.36743345 0.82774046 0.         0.        ]\n",
      " [0.         0.35088001 0.         0.68487548 0.56160769 0.\n",
      "  0.30401578 0.         0.         0.        ]\n",
      " [0.         0.         0.67465286 0.         0.         0.\n",
      "  0.29947796 0.         0.67465286 0.        ]\n",
      " [0.68487548 0.35088001 0.         0.         0.56160769 0.\n",
      "  0.30401578 0.         0.         0.        ]\n",
      " [0.         0.42407356 0.         0.         0.         0.82774046\n",
      "  0.36743345 0.         0.         0.        ]]\n",
      "['brula' 'chat' 'chaud' 'mit' 'patte' 'quitta' 'rot' 'tenta' 'trop' 'vit']\n"
     ]
    }
   ],
   "source": [
    "docs_raw_sample = [\"Chat vit rôt.\",\n",
    "                   \"Rôt tenta chat.\",\n",
    "                   \"Chat mit patte à rôt.\",\n",
    "                   \"Rôt trop chaud !\",\n",
    "                   \"Rôt brûla patte à chat.\",\n",
    "                   \"Chat quitta rôt.\"]\n",
    "\n",
    "# Write code to import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Write code to convert the list of documents to list of tokens.\n",
    "docs_raw_sample_tfidf = [raw_to_tokens(docs_raw_sample[k], spacy_nlp) for k in range(len(docs_raw_sample))]\n",
    "\n",
    "# Write code to create a TfidfVectorizer object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Write code to vectorize the sample text\n",
    "X_tfidf_sample = tfidf.fit_transform(docs_raw_sample_tfidf)\n",
    "\n",
    "print(\"Shape of the TF-IDF Matrix:\")\n",
    "print(X_tfidf_sample.shape)\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(X_tfidf_sample.todense())\n",
    "print(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLNJtiVvDyhd"
   },
   "source": [
    "## From Documents to Features\n",
    "TF-IDF basically transforms a set of documents into a set of features, which can be directly used in machine learning tasks.\n",
    "\n",
    "Let's now convert EU Parliamanet documents into Tf-Idf vectors.\n",
    "\n",
    "A correct conversion will generate a matrix of dimensions (10000, 13429)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T15:09:55.808046Z",
     "start_time": "2024-05-15T15:09:55.742803Z"
    },
    "id": "Vx4Y7sgpDyhd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF Matrix:\n",
      "(10000, 13467)\n"
     ]
    }
   ],
   "source": [
    "# Write code to convert raw documents into TF-IDF matrix.\n",
    "\"\"\"\n",
    "Hint: - create a TfidfVectorizer for clean EU Parliamanet documents\n",
    "      - use fit_transform to vectorize raw_docs\n",
    "\"\"\"\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(docs_clean)\n",
    "\n",
    "print(\"Shape of the TF-IDF Matrix:\")\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smLupsOzDyhe"
   },
   "source": [
    "## Course Project: Text Classification with Rakuten France Product Data\n",
    "\n",
    "The project focuses on the topic of large-scale product type code text classification where the goal is to predict each product’s type code as defined in the catalog of Rakuten France. This project is derived from a data challenge proposed by Rakuten Institute of Technology, Paris. Details of the data challenge is [available in this link](https://challengedata.ens.fr/challenges/35).\n",
    "\n",
    "The above data challenge focuses on multimodal product type code classification using text and image data. **For this project we will work with only text part of the data.**\n",
    "\n",
    "Please read carefully the description of the challenge provided in the above link. **You can disregard any information related to the image part of the data.**\n",
    "\n",
    "### To obtain the data\n",
    "You have to register yourself [in this link](https://challengedata.ens.fr/challenges/35) to get access to the data.\n",
    "\n",
    "For this project you will only need the text data. Download the training files `x_train` and `y_train`, containing the item texts, and the corresponding product type code labels.\n",
    "\n",
    "### Pandas for handling the data\n",
    "The files you obtained are in CSV format. We strongly suggest to use Python Pandas package to load and visualize the data. [Here is a basic tutorial](https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/) on how to handle data in CSV file using Pandas.\n",
    "\n",
    "If you open the `x_train` dataset using Pandas, you will find that it contains following columns:\n",
    "1. an integer ID for the product\n",
    "2. **designation** - The product title\n",
    "3. description\n",
    "4. productid\n",
    "5. imageid\n",
    "\n",
    "For this project we will only need the integer ID and the designation. You can [`drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) the other columns.\n",
    "\n",
    "The training output file `y_train.csv` contains the **prdtypecode**, the target/output variable for the classification task, for each integer id in the training input file `X_train.csv`.\n",
    "\n",
    "### Task for the break\n",
    "1. Register yourself and download the training and test for text data. You do not need the `supplementary files` for this project.\n",
    "2. Load the data using pandas and disregard unnecessary columns as mentioned above.\n",
    "3. On the **designation** column, apply the preprocessing techniques.\n",
    "\n",
    "### Task for the end of the course\n",
    "After this preprocessing step, you have now access to a TF-IDF matrix that constitute our data set for the final evaluation project. The project guidelines are:\n",
    "1. Apply all appropriated approaches taught in the course and practiced in lab sessions on this data set. The goal is to predict the target variable (prdtypecode).\n",
    "2. Compare performances of all these models in terms of the weighted-f1 scores you can output.\n",
    "3. Conclude about the most appropriate approach on this data set for the predictive task.\n",
    "4. Write a report that adress all these guidelines with a maximal page number of 5 (including figures, tables and references). We will take into account the quality of writing and presentation of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T15:27:31.821400Z",
     "start_time": "2024-05-15T15:27:31.782391Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
